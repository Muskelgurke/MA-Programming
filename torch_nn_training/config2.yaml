# Base Configuration: Grundeinstellungen für alle Trainings
base_config:
  random_seed: 43
  learning_rate: 0.00125
  epoch_num: 10
  training_method: fgd
  dataset_name: cifar10
  batch_size: 64
  augment_data: false
  model_type: cnn
  optimizer: adam
  momentum: 0.9
  early_stopping: true
  early_stopping_patience: 5
  early_stopping_delta: 0.01
  dataset_path: _Dataset

# Multi-Parameter Configuration: Parameter für Grid-Search
# Wenn ein Parameter hier definiert ist, wird er für Multi-Training verwendet
# Wenn nicht, wird der Wert aus base_config genommen

multi_params:
  
  learning_rate:
    - 0.000175
    - 0.00015
    - 0.0002
    - 0.000225
    - 0.00025
    - 0.000275
    - 0.0003
    - 0.0004
    - 0.0005
    - 0.0006
    - 0.0007
    - 0.0008
    - 0.0009
    - 0.001
    - 0.00125
    - 0.0015
  training_method:
    - fgd

  random_seed:
    - 42
    - 69
    - 103
    - 256
  optimizer:
    - sgd
    - adam

  #momentum:
  #  - 0.8
  #  - 0.9
  #  - 0.95
  #  - 0.99
  #batch_size:
  #  - 50
  #  - 64
  #  - 100
  #  - 128
  #  - 200
  #  - 256
  # Optional: Weitere Parameter
  # batch_size:
  #   - 32
  #   - 64

  # epoch_num:
  #   - 5
  #   - 10

# Beispiel für verschiedene Setups:
# 1. Nur Learning Rate testen:
#    learning_rate: [0.001, 0.0001]
#    # Alle anderen aus base_config
#
# 2. Vollständiger Grid-Search:
#    learning_rate: [0.01, 0.001, 0.0001]
#    training_method: [bp, fgd]
#    random_seed: [42, 43, 44]
#    batch_size: [32, 64]
#    # = 3 × 2 × 3 × 2 = 36 Kombinationen
